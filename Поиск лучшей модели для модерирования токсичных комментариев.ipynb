{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Обзор-данных\" data-toc-modified-id=\"Обзор-данных-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Обзор данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Подготовка данных</a></span></li></ul></li><li><span><a href=\"#Обучение-и-тестирование\" data-toc-modified-id=\"Обучение-и-тестирование-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение и тестирование</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регресия\" data-toc-modified-id=\"Логистическая-регресия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регресия</a></span></li><li><span><a href=\"#SGDClassifier\" data-toc-modified-id=\"SGDClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>SGDClassifier</a></span></li><li><span><a href=\"#CatBoostClassifier\" data-toc-modified-id=\"CatBoostClassifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>CatBoostClassifier</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "toxic_comments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n",
      "-----------------------------------------------------------------\n",
      "Дубликатов - 0\n",
      "-----------------------------------------------------------------\n",
      "Пропусков:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Соотношение в целевом признаке:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(toxic_comments.head(5))\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "toxic_comments.info()\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Дубликатов -', toxic_comments.duplicated().sum())\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Пропусков:')\n",
    "display(toxic_comments.isna().sum())\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "print('Соотношение в целевом признаке:')\n",
    "display(toxic_comments.toxic.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**<br>\n",
    "- В таблице 159 292 объектов. Пропусков нет, явных дубликатов нет\n",
    "- Тексты комментариев на английском, есть лишние символы, например `\\n`\n",
    "- В целевом признаке ~90% объектов отрицательного класса, то есть в дальнейшем нужно будет учесть это\n",
    "- Необходимо избавиться от столбца `Unnamed`, так как он фактически дублирует индексы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляем столбец Unnamed:\n",
    "toxic_comments = toxic_comments.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вводим функцию очищения текстов постов:\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 27.5 ms, total: 4.15 s\n",
      "Wall time: 4.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#очищаем тексты постов:\n",
    "toxic_comments['text'] = toxic_comments['text'].apply(clear_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  explanation why the edits made under my userna...      0\n",
       "1  d aww he matches this background colour i m se...      0\n",
       "2  hey man i m really not trying to edit war it s...      0\n",
       "3  more i can t make any real suggestions on impr...      0\n",
       "4  you sir are my hero any chance you remember wh...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вводим функцию РОS-тэгирования слов:\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,               #прилагательное\n",
    "                \"N\": wordnet.NOUN,              #существительное\n",
    "                \"V\": wordnet.VERB,              #глагол\n",
    "                \"R\": wordnet.ADV                #наречие\n",
    "               }  \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#вводим функцию леммализации тектов постов:\n",
    "def lemm_text(text):\n",
    "    text = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 13s, sys: 1min 32s, total: 18min 45s\n",
      "Wall time: 18min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#леммализируем тексты постов:\n",
    "toxic_comments['text'] = toxic_comments['text'].apply(lemm_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  explanation why the edits make under my userna...      0\n",
       "1  d aww he match this background colour i m seem...      0\n",
       "2  hey man i m really not try to edit war it s ju...      0\n",
       "3  more i can t make any real suggestion on impro...      0\n",
       "4  you sir be my hero any chance you remember wha...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделяем выборки в соотношении 80/10/10:\n",
    "features = toxic_comments.drop(['toxic'], axis=1) \n",
    "target = toxic_comments['toxic']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=.2, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_valid, \n",
    "                                                                            target_valid, \n",
    "                                                                            test_size=.5,\n",
    "                                                                            random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127433, 1)\n",
      "(127433,)\n",
      "(15929, 1)\n",
      "(15929,)\n",
      "(15930, 1)\n",
      "(15930,)\n"
     ]
    }
   ],
   "source": [
    "#глянем размеры выборок:\n",
    "for i in [features_train, target_train, features_valid, target_valid, features_test, target_test]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля значений 1 в тренировочной выборке: 0.10166911239631807\n"
     ]
    }
   ],
   "source": [
    "#глянем соотношение классов в выборках на примере target_train:\n",
    "indices_1 = [i for i,x in enumerate(target_train) if x == 1]\n",
    "count_1 = len(indices_1)\n",
    "\n",
    "indices_0 = [i for i,x in enumerate(target_train) if x == 0]\n",
    "count_0 = len(indices_0)\n",
    "\n",
    "print('Доля значений 1 в тренировочной выборке:', len(indices_1) / (len(indices_1) + len(indices_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Соотношение 1/0 в тренировочной выборке:\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: toxic, dtype: float64\n",
      "\n",
      "(25912,)\n",
      "(25912,)\n"
     ]
    }
   ],
   "source": [
    "#снизим кол-во 0 в выборках train:\n",
    "\n",
    "toxic_comments_train = toxic_comments.iloc[target_train.index]\n",
    "target_train_0 = toxic_comments_train[toxic_comments_train['toxic'] == 0]['toxic']\n",
    "target_train_1 = toxic_comments_train[toxic_comments_train['toxic'] == 1]['toxic']\n",
    "\n",
    "\n",
    "target_train_0_resample = target_train_0.sample(target_train_1.shape[0], random_state=12345)\n",
    "target_train_resample = pd.concat([target_train_0_resample, target_train_1])\n",
    "\n",
    "features_train_resample = toxic_comments.iloc[target_train_resample.index]\n",
    "\n",
    "features_train_resample, target_train_resample = shuffle(features_train_resample,\n",
    "                                                         target_train_resample,\n",
    "                                                         random_state=12345)\n",
    "\n",
    "features_train_resample = features_train_resample.text \n",
    "\n",
    "print('Соотношение 1/0 в тренировочной выборке:')\n",
    "print(target_train_resample.value_counts(normalize=True))\n",
    "print()\n",
    "print(features_train_resample.shape)\n",
    "print(target_train_resample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**<br>\n",
    "- Удалили ненужный столбец `Unnamed`\n",
    "- Очистили тексты комментариев от ненужных знаков, леммализировали и убрали стоп-слова\n",
    "- Сбалансировали данные по значениям классов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регресия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Логистической регрессии = 0.77\n",
      "при параметрах {'lr__C': 10, 'lr__max_iter': 200, 'lr__random_state': 12345, 'lr__solver': 'saga'}\n",
      "\n",
      "F1 логистической регрессии на валидации = 0.78\n",
      "\n",
      "финальный F1 логистической регрессии = 0.79\n",
      "CPU times: user 11min 32s, sys: 7min 54s, total: 19min 26s\n",
      "Wall time: 19min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english', sublinear_tf=True)), \n",
    "                     (\"lr\", LogisticRegression())])\n",
    "    \n",
    "parameters = {'lr__solver': ('liblinear', 'saga','newton-cg', 'lbfgs'),\n",
    "              'lr__C': (.1, 1, 5, 10),\n",
    "              'lr__random_state': ([12345]),\n",
    "              'lr__max_iter': ([200]),} \n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv.fit(features_train, target_train)\n",
    "\n",
    "mts = gscv.cv_results_['mean_test_score']\n",
    "lr_train_f1 = max(mts)\n",
    "\n",
    "print('F1 Логистической регрессии =', round(lr_train_f1,2))\n",
    "print('при параметрах', gscv.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv.predict(features_valid.text)\n",
    "lr_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 логистической регрессии на валидации =', round(lr_valid_f1,2))\n",
    "print()\n",
    "\n",
    "#тестирование:\n",
    "predictions_test = gscv.predict(features_test.text)\n",
    "lr_test_f1 = f1_score(target_test, predictions_test)\n",
    "print('финальный F1 логистической регрессии =', round(lr_test_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 SGDClassifier = 0.73\n",
      "при параметрах {'clf__eta0': 0.05, 'clf__learning_rate': 'constant', 'clf__loss': 'modified_huber', 'clf__random_state': 12345}\n",
      "\n",
      "F1 SGDClassifier на валидации = 0.73\n",
      "\n",
      "финальный F1 SGDClassifier = 0.74\n",
      "CPU times: user 17min 56s, sys: 18 s, total: 18min 14s\n",
      "Wall time: 18min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"clf\", SGDClassifier())])\n",
    "    \n",
    "parameters = {'clf__loss': ('hinge', 'log', 'modified_huber'),\n",
    "              'clf__learning_rate': ('constant', 'optimal', 'invscaling', 'adaptive'),\n",
    "              'clf__eta0': (.01, .05, .1, .5),\n",
    "              'clf__random_state': ([12345])}\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv.fit(features_train, target_train)\n",
    "\n",
    "mts = gscv.cv_results_['mean_test_score']\n",
    "sgdc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 SGDClassifier =', round(sgdc_train_f1,2))\n",
    "print('при параметрах', gscv.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv.predict(features_valid.text)\n",
    "sgdc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 SGDClassifier на валидации =', round(sgdc_valid_f1,2))\n",
    "print()\n",
    "\n",
    "#тестирование:\n",
    "predictions_test = gscv.predict(features_test.text)\n",
    "sgdc_test_f1 = f1_score(target_test, predictions_test)\n",
    "print('финальный F1 SGDClassifier =', round(sgdc_test_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 CatBoostClassifier = 0.74\n",
      "при параметрах {'cbc__iterations': 200, 'cbc__verbose': False}\n",
      "\n",
      "F1 CatBoostClassifier на валидации = 0.76\n",
      "\n",
      "финальный F1 CatBoostClassifier = 0.75\n",
      "CPU times: user 25min 6s, sys: 5min 19s, total: 30min 26s\n",
      "Wall time: 30min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#обучение:\n",
    "pipeline = Pipeline([(\"vect\", TfidfVectorizer(stop_words='english')), \n",
    "                     (\"cbc\", CatBoostClassifier())])\n",
    "    \n",
    "parameters = {'cbc__verbose': ([False]),\n",
    "              'cbc__iterations': ([200])} \n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "gscv.fit(features_train, target_train)\n",
    "\n",
    "mts = gscv.cv_results_['mean_test_score']\n",
    "cbc_train_f1 = max(mts)\n",
    "\n",
    "print('F1 CatBoostClassifier =', round(cbc_train_f1,2))\n",
    "print('при параметрах', gscv.best_params_)\n",
    "print()\n",
    "\n",
    "#валидация:\n",
    "predictions_valid = gscv.predict(features_valid.text)\n",
    "cbc_valid_f1 = f1_score(target_valid, predictions_valid)\n",
    "print('F1 CatBoostClassifier на валидации =', round(cbc_valid_f1,2))\n",
    "print()\n",
    "\n",
    "#тестирование:\n",
    "predictions_test = gscv.predict(features_test.text)\n",
    "cbc_test_f1 = f1_score(target_test, predictions_test)\n",
    "print('финальный F1 CatBoostClassifier =', round(cbc_test_f1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 на обучающей выборке</th>\n",
       "      <th>F1 на валидационной выборке</th>\n",
       "      <th>F1 на тестовой выборке (финальный)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.767863</td>\n",
       "      <td>0.78153</td>\n",
       "      <td>0.789256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.736849</td>\n",
       "      <td>0.75904</td>\n",
       "      <td>0.750541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.731007</td>\n",
       "      <td>0.72900</td>\n",
       "      <td>0.735615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    F1 на обучающей выборке  F1 на валидационной выборке  \\\n",
       "LogisticRegression                 0.767863                      0.78153   \n",
       "CatBoostClassifier                 0.736849                      0.75904   \n",
       "SGDClassifier                      0.731007                      0.72900   \n",
       "\n",
       "                    F1 на тестовой выборке (финальный)  \n",
       "LogisticRegression                            0.789256  \n",
       "CatBoostClassifier                            0.750541  \n",
       "SGDClassifier                                 0.735615  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#создадим сводную таблицу по показателям F1:\n",
    "index = ['LogisticRegression',\n",
    "         'CatBoostClassifier',\n",
    "         'SGDClassifier',\n",
    "        ]\n",
    "\n",
    "data = {'F1 на обучающей выборке': [lr_train_f1,\n",
    "                                    cbc_train_f1,\n",
    "                                    sgdc_train_f1],\n",
    "        \n",
    "        'F1 на валидационной выборке': [lr_valid_f1,\n",
    "                                        cbc_valid_f1,\n",
    "                                        sgdc_valid_f1],\n",
    "        \n",
    "        'F1 на тестовой выборке (финальный)': [lr_test_f1,\n",
    "                                               cbc_test_f1,\n",
    "                                               sgdc_test_f1]}\n",
    "\n",
    "f1_data = pd.DataFrame(data=data, index=index)\n",
    "\n",
    "f1_data.sort_values(by='F1 на тестовой выборке (финальный)', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** <br>\n",
    "- загрузили данные и провели их предобработку - удаление лишних данных, очистку текстов, лемматизацию\n",
    "- обучили 3 модели с разными гиперпараметрами и выборками и проверили их на тестовой выборке\n",
    "- выбрали лучшую модель по показателю F1\n",
    "\n",
    "Аутсайдером среди моделей стал SGDClassifier, так как дал наименьший F1 (0.73 на обучающей и тестовой выборке).\n",
    "\n",
    "Наилучшей моделью стала LogisticRegression, которая на тестировании показала F1 = 0.79. Поскольку требовалось найти модель классификации комментариев на позитивные и негативные со значением метрики качества F1 >= 0.75 после обучения, смело рекоммендуем LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2127,
    "start_time": "2023-09-28T11:54:50.837Z"
   },
   {
    "duration": 3525,
    "start_time": "2023-09-28T11:55:02.416Z"
   },
   {
    "duration": 30,
    "start_time": "2023-09-28T11:55:20.566Z"
   },
   {
    "duration": 9,
    "start_time": "2023-09-28T11:55:49.868Z"
   },
   {
    "duration": 37,
    "start_time": "2023-09-28T11:57:56.124Z"
   },
   {
    "duration": 983,
    "start_time": "2023-09-28T11:58:03.806Z"
   },
   {
    "duration": 30,
    "start_time": "2023-09-28T11:58:06.476Z"
   },
   {
    "duration": 5,
    "start_time": "2023-09-28T11:58:09.676Z"
   },
   {
    "duration": 286,
    "start_time": "2023-09-28T11:58:49.294Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-28T12:02:59.645Z"
   },
   {
    "duration": 9,
    "start_time": "2023-09-28T12:03:15.107Z"
   },
   {
    "duration": 150,
    "start_time": "2023-09-28T12:03:30.974Z"
   },
   {
    "duration": 9,
    "start_time": "2023-09-28T12:03:39.474Z"
   },
   {
    "duration": 12,
    "start_time": "2023-09-28T12:03:54.605Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:04:13.884Z"
   },
   {
    "duration": 989,
    "start_time": "2023-09-28T12:04:13.890Z"
   },
   {
    "duration": 319,
    "start_time": "2023-09-28T12:04:14.881Z"
   },
   {
    "duration": 10,
    "start_time": "2023-09-28T12:04:18.635Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-28T12:04:39.074Z"
   },
   {
    "duration": 4246,
    "start_time": "2023-09-28T12:04:56.867Z"
   },
   {
    "duration": 8,
    "start_time": "2023-09-28T12:05:18.274Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:07:00.164Z"
   },
   {
    "duration": 501,
    "start_time": "2023-09-28T12:09:29.510Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:12:35.799Z"
   },
   {
    "duration": 502,
    "start_time": "2023-09-28T12:12:38.898Z"
   },
   {
    "duration": 316,
    "start_time": "2023-09-28T12:13:37.076Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:13:48.917Z"
   },
   {
    "duration": 1110793,
    "start_time": "2023-09-28T12:13:51.267Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-28T12:32:22.062Z"
   },
   {
    "duration": 78,
    "start_time": "2023-09-28T12:32:29.550Z"
   },
   {
    "duration": 40,
    "start_time": "2023-09-28T12:32:35.400Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:32:37.372Z"
   },
   {
    "duration": 33,
    "start_time": "2023-09-28T12:32:45.340Z"
   },
   {
    "duration": 48,
    "start_time": "2023-09-28T12:32:51.729Z"
   },
   {
    "duration": 6,
    "start_time": "2023-09-28T12:33:05.777Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T12:33:18.466Z"
   },
   {
    "duration": 6,
    "start_time": "2023-09-28T12:33:23.569Z"
   },
   {
    "duration": 337486,
    "start_time": "2023-09-28T12:35:24.637Z"
   },
   {
    "duration": 573248,
    "start_time": "2023-09-28T12:41:02.125Z"
   },
   {
    "duration": 336352,
    "start_time": "2023-09-28T12:50:35.375Z"
   },
   {
    "duration": 24,
    "start_time": "2023-09-28T12:56:11.729Z"
   },
   {
    "duration": 702,
    "start_time": "2023-09-28T13:23:27.070Z"
   },
   {
    "duration": 263,
    "start_time": "2023-09-28T14:20:43.489Z"
   },
   {
    "duration": 921,
    "start_time": "2023-09-28T14:20:43.753Z"
   },
   {
    "duration": 282,
    "start_time": "2023-09-28T14:20:44.675Z"
   },
   {
    "duration": 23,
    "start_time": "2023-09-28T14:20:44.958Z"
   },
   {
    "duration": 59,
    "start_time": "2023-09-28T14:20:44.984Z"
   },
   {
    "duration": 4102,
    "start_time": "2023-09-28T14:20:45.045Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-28T14:20:49.149Z"
   },
   {
    "duration": 10,
    "start_time": "2023-09-28T14:20:49.157Z"
   },
   {
    "duration": 1124335,
    "start_time": "2023-09-28T14:20:49.169Z"
   },
   {
    "duration": 9,
    "start_time": "2023-09-28T14:39:33.506Z"
   },
   {
    "duration": 92,
    "start_time": "2023-09-28T14:39:33.516Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-28T14:39:33.610Z"
   },
   {
    "duration": 53,
    "start_time": "2023-09-28T14:39:33.615Z"
   },
   {
    "duration": 56,
    "start_time": "2023-09-28T14:39:33.671Z"
   },
   {
    "duration": 364220,
    "start_time": "2023-09-28T14:46:58.049Z"
   },
   {
    "duration": 4037,
    "start_time": "2023-09-28T16:44:19.299Z"
   },
   {
    "duration": 4446,
    "start_time": "2023-09-28T16:44:23.338Z"
   },
   {
    "duration": 350,
    "start_time": "2023-09-28T16:44:27.785Z"
   },
   {
    "duration": 33,
    "start_time": "2023-09-28T16:44:28.138Z"
   },
   {
    "duration": 10,
    "start_time": "2023-09-28T16:44:28.180Z"
   },
   {
    "duration": 5825,
    "start_time": "2023-09-28T16:44:28.195Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-28T16:44:34.022Z"
   },
   {
    "duration": 37,
    "start_time": "2023-09-28T16:44:34.031Z"
   },
   {
    "duration": 1804066,
    "start_time": "2023-09-28T16:44:34.076Z"
   },
   {
    "duration": 10,
    "start_time": "2023-09-28T17:14:38.145Z"
   },
   {
    "duration": 75,
    "start_time": "2023-09-28T17:14:38.157Z"
   },
   {
    "duration": 6,
    "start_time": "2023-09-28T17:14:38.241Z"
   },
   {
    "duration": 110,
    "start_time": "2023-09-28T17:14:38.249Z"
   },
   {
    "duration": 96,
    "start_time": "2023-09-28T17:14:38.367Z"
   },
   {
    "duration": 162,
    "start_time": "2023-09-28T17:22:52.990Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-28T17:22:59.024Z"
   },
   {
    "duration": 6,
    "start_time": "2023-09-28T17:23:10.524Z"
   },
   {
    "duration": 947,
    "start_time": "2023-09-28T17:46:26.699Z"
   },
   {
    "duration": 8856,
    "start_time": "2023-09-28T17:46:57.819Z"
   },
   {
    "duration": 14,
    "start_time": "2023-09-28T17:50:01.818Z"
   },
   {
    "duration": 11,
    "start_time": "2023-09-28T17:50:13.173Z"
   },
   {
    "duration": 42,
    "start_time": "2023-09-28T17:50:19.996Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-28T17:50:33.644Z"
   },
   {
    "duration": 1220233,
    "start_time": "2023-09-28T17:50:40.693Z"
   },
   {
    "duration": 3635488,
    "start_time": "2023-09-28T18:17:42.134Z"
   },
   {
    "duration": 2682,
    "start_time": "2023-09-29T06:23:09.998Z"
   },
   {
    "duration": 2434,
    "start_time": "2023-09-29T06:23:12.682Z"
   },
   {
    "duration": 313,
    "start_time": "2023-09-29T06:23:15.118Z"
   },
   {
    "duration": 15,
    "start_time": "2023-09-29T06:23:15.433Z"
   },
   {
    "duration": 28,
    "start_time": "2023-09-29T06:23:15.451Z"
   },
   {
    "duration": 4173,
    "start_time": "2023-09-29T06:23:15.480Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-29T06:23:19.654Z"
   },
   {
    "duration": 36,
    "start_time": "2023-09-29T06:23:19.663Z"
   },
   {
    "duration": 1126334,
    "start_time": "2023-09-29T06:23:19.701Z"
   },
   {
    "duration": 7,
    "start_time": "2023-09-29T06:42:06.037Z"
   },
   {
    "duration": 48,
    "start_time": "2023-09-29T06:42:06.046Z"
   },
   {
    "duration": 4,
    "start_time": "2023-09-29T06:42:06.096Z"
   },
   {
    "duration": 61,
    "start_time": "2023-09-29T06:42:06.102Z"
   },
   {
    "duration": 58,
    "start_time": "2023-09-29T06:42:06.166Z"
   },
   {
    "duration": 3,
    "start_time": "2023-09-29T06:42:06.226Z"
   },
   {
    "duration": 1167649,
    "start_time": "2023-09-29T06:42:06.231Z"
   },
   {
    "duration": 3367270,
    "start_time": "2023-09-29T07:01:33.883Z"
   },
   {
    "duration": 1834957,
    "start_time": "2023-09-29T07:57:41.155Z"
   },
   {
    "duration": 21,
    "start_time": "2023-09-29T08:28:16.115Z"
   },
   {
    "duration": 920545,
    "start_time": "2023-09-29T08:31:46.164Z"
   },
   {
    "duration": 983015,
    "start_time": "2023-09-29T08:48:32.716Z"
   },
   {
    "duration": 333,
    "start_time": "2023-09-29T09:05:44.347Z"
   },
   {
    "duration": 1099605,
    "start_time": "2023-09-29T09:06:37.618Z"
   },
   {
    "duration": 20,
    "start_time": "2023-09-29T09:24:57.229Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
